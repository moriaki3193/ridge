# Hodwild!
このメモは[1]の翻訳記事に近いものである．
Field-aware Factorization Machineの実装の一つである[2]では，モデルの学習プロセスを高速化する為にHogwild!の手法を利用している．
しばしばモデルの学習には長い時間を費やし，そのために試行錯誤のプロセスに割く時間が削られてしまう．
そのような経験を緩和する為に，一つのアプローチとしてこの手法に対する理解を深め，`ridge`をより良いものにすることを目指す．

## Overview
Hogwild!は非同期的な確率的勾配降下法のアルゴリズムの一つである．
この手法では，`lock-free`(訳注: ある勾配を計算する場合に，その他の勾配の計算を妨げることをしないよう)な勾配の更新アプローチをとる．
機械学習のモデルの観点からすれば，重みは複数のプロセスから同時に**互いに上書きされる可能性を伴って**更新されうることを意味している．
この記事では，Hogwild!をPythonで実装するために，`multiprocessing`[3]ライブラリを利用し，線形回帰モデルを学習させる方法について紹介したいと思う．

あまりに難しく込み入ったことを紹介してこの記事の読者を混乱させてしまうことがないように，読者はすでに勾配降下法のコンセプトと，非同期的プログラミングがどのようなものかをある程度把握しているものとする．

## Hogwild! Explained
勾配降下法は反復的なアルゴリズムである．処理を高速化するためには，ひょっとしたらGPUなどのより高速な処理系にその反復的な処理を投げる必要があるかもしれない．
Hogwild!は確率的勾配降下法の手法をより大きなデータセットを利用した学習に対しても有効にするための，並行的な手法だ．
その基本的なアルゴリズムは一見してシンプルなものである．
前提とする条件は，ニューラルネットワークであれ，線形回帰であれ，マシンに搭載されるプロセッサが重みを同時に読み込めて，なおかつ更新できるということである．
端的に言えば，重みがこれ以上分割できない構成要素の加算が可能な形式で共有メモリに格納されていることが必要ということだ．
この手法が提案された論文から引用すれば，個々のプロセッサの確率的勾配降下法による重みの更新は，次のような擬似コードで表現される．

```
loop
    トレーニング集合から一つの事例 (X_i, y_i) を取り出す
    (X_i, y_i) における勾配を評価する
    要素ごとに重みの更新を行う
end loop
```

それでは，このコードを複数のプロセッサで走らせ，同じモデルの重みを同時に更新する．
ランダムに動き回り，互いのつま先を踏みつけあっている数多くの農場の動物を監視しているような挙動から，
このアルゴリズムがHogwild!と呼ばれる所以がわかるだろう．

このアルゴリズムが実現するための条件について今回我々は特に重きを置かないが，いくつかあるうち大抵重要なものは，
勾配の更新はスパースであり，勾配の更新が行われる0ではない要素の数が少数でなければならないことだ．
これは学習に放り込むデータがスパースである場合によく当てはまるだろう．
更新の頻度が疎であるということは，異なるプロセッサが互いのつま先を踏みつけ合うことが少ないということである．
換言すれば，同じ重みを更新するために，同時にその重みにアクセスすることが少ないということだ．
ただし，その可能性が比較的小さいというだけで，衝突(訳注: 同じ重みに同時にアクセスすること)は起こる．
しかしながら，実際にはそれが正則化の役割を担うものとして解釈できるのである．

エクササイズに当たって，線形回帰モデルをHogwild!を利用した学習を実装していく．
[GitHubのリポジトリ](https://github.com/srome/sklearn-hogwild)には，scikit-learnを有効活用した実装が全て掲載されている．
この投稿においては，リポジトリのコードの一部を抜粋するので，適せんリポジトリの内容を参照してほしい．

## Linear Regression Refresher
私と読者の間で共通の認識ができるように，ここでは以降の説明で利用する記法について説明したい．
線形回帰モデル*f*は，*n*次元の実数ベクトル`x`を入力として受け取り，実数`y`を以下のような式を通じて出力する．

```TeX
f(\mathbf{x}) = \mathbf{w} \cdot \mathbf{x}
```

上の式における`w`が、このモデルにおいて学習させたい重みのベクトルである．
損失関数としては，$2$乗誤差を利用し，データセットの一つの事例に対する損失は次のように書き表すことができる．

```TeX
l(\mathbf{x}, y) = \left(f(\mathbf{x}) - y\right)^{2}
```

確率的勾配降下法を通じてモデルを学習させるために，以下のように勾配更新ステップを計算する必要がある．

```TeX
\mathbf{w}_{t+1} = \mathbf{w}_{t} - \lambda G_{w}(\mathbf{x}, y)
```

ここでいう$\lambda$とは学習率であり，$G_{w}$は損失$l$の$\mathbf{w}$についての勾配の期待値である．すなわち

```TeX
\mathcal{E}[G_{w}(\mathbf{x}, y)] = \nabra_{w}l(\mathbf{x}, y)
```

特に，損失が$2$乗誤差である場合には，次のように計算できる．

```TeX
G_{\mathbf{w}}(\mathbf{w}, y) = -2 \left(\mathbf{w}\cdot\mathbf{x} - y)\mathbf{x} \in \mathcal{R}
```

## Generating our Training Data

### High Level Approach

### Lock-free Shared Memory in Python

### Gradient Update

### Preparing the examples for multiprocessing.Pool

### The Asyncronous Bit

## Importance of Asynchronous Methods

## Reference
1. [Hogwild!? Implementing Async SGD in Python](https://srome.github.io/Async-SGD-in-Python-Implementing-Hogwild!/)
2. [libFFM - A Library for Field-aware Factorization Machines](https://github.com/guestwalk/libffm)
3. [mulitprocessing - Python](https://docs.python.jp/3/library/multiprocessing.html)